{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"YOLOs-CPP","text":"<p>     Modern C++ YOLO Library   </p> <p>Welcome to the official documentation for YOLOs-CPP - a high-performance, production-ready C++ library for object detection.</p>"},{"location":"#quick-navigation","title":"Quick Navigation","text":"Section Description Getting Started Installation and first steps API Reference Complete API documentation Tutorials Step-by-step tutorials Examples Code examples YOLOs Available YOLO models Architecture System design Benchmarking Benchmark evaluation Contributing How to contribute"},{"location":"#features","title":"Features","text":"<ul> <li>Multiple YOLO Models \u2014 YOLOv8 to YOLOv26</li> <li>Multiple Tasks \u2014 Detection, Segmentation, Pose Estimation</li> <li>High Performance \u2014 Optimized C++17 implementation</li> <li>Modern API \u2014 Clean, intuitive interface with ONNX Runtime and OpenCV</li> <li>Cross-Platform \u2014 Linux, macOS, Windows</li> <li>Well Tested \u2014 Comprehensive unit tests</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\n#include &lt;chrono&gt;\n#include &lt;filesystem&gt;\n#include &lt;vector&gt;\n#include \"yolos/tasks/detection.hpp\"\n#include \"utils.hpp\"\n\nusing namespace yolos::det;\n\nint main(int argc, char* argv[]) {\n    namespace fs = std::filesystem;\n\n    // Default configuration\n    std::string modelPath = \"../../models/yolo11n.onnx\";\n    std::string inputPath = \"../../data/dog.jpg\";\n    std::string labelsPath = \"../../models/coco.names\";\n    std::string outputDir = \"../../outputs/det/\";\n\n    // Check for help flag\n    if (argc &gt; 1 &amp;&amp; (std::string(argv[1]) == \"--help\" || std::string(argv[1]) == \"-h\")) {\n        utils::printUsage(argv[0], \"Object Detection\", modelPath, inputPath, labelsPath);\n        return 0;\n    }\n\n    // Parse command line arguments\n    if (argc &gt; 1) modelPath = argv[1];\n    if (argc &gt; 2) inputPath = argv[2];\n    if (argc &gt; 3) labelsPath = argv[3];\n\n    // Print usage information\n    utils::printUsage(argv[0], \"Object Detection\", modelPath, inputPath, labelsPath);\n\n    // Collect image files\n    std::vector&lt;std::string&gt; imageFiles;\n    try {\n        if (fs::is_directory(inputPath)) {\n            for (const auto&amp; entry : fs::directory_iterator(inputPath)) {\n                if (entry.is_regular_file() &amp;&amp; utils::isImageFile(entry.path().string())) {\n                    imageFiles.push_back(fs::absolute(entry.path()).string());\n                }\n            }\n            if (imageFiles.empty()) {\n                std::cerr &lt;&lt; \"\u274c No image files found in: \" &lt;&lt; inputPath &lt;&lt; std::endl;\n                return -1;\n            }\n        } else if (fs::is_regular_file(inputPath)) {\n            imageFiles.push_back(inputPath);\n        } else {\n            std::cerr &lt;&lt; \"\u274c Invalid path: \" &lt;&lt; inputPath &lt;&lt; std::endl;\n            return -1;\n        }\n    } catch (const fs::filesystem_error&amp; e) {\n        std::cerr &lt;&lt; \"\u274c Filesystem error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n\n    // Initialize YOLO detector\n    bool useGPU = false; // CPU by default\n    std::cout &lt;&lt; \"\ud83d\udd04 Loading detection model: \" &lt;&lt; modelPath &lt;&lt; std::endl;\n\n    try {\n        YOLODetector detector(modelPath, labelsPath, useGPU);\n        std::cout &lt;&lt; \"\u2705 Model loaded successfully!\" &lt;&lt; std::endl;\n\n        // Process each image\n        for (const auto&amp; imgPath : imageFiles) {\n            std::cout &lt;&lt; \"\\n\ud83d\udcf7 Processing: \" &lt;&lt; imgPath &lt;&lt; std::endl;\n\n            // Load image\n            cv::Mat image = cv::imread(imgPath);\n            if (image.empty()) {\n                std::cerr &lt;&lt; \"\u274c Could not load image: \" &lt;&lt; imgPath &lt;&lt; std::endl;\n                continue;\n            }\n\n            // Run detection with timing\n            auto start = std::chrono::high_resolution_clock::now();\n            std::vector&lt;Detection&gt; detections = detector.detect(image);\n            auto duration = std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(\n                std::chrono::high_resolution_clock::now() - start);\n\n            // Print results\n            std::cout &lt;&lt; \"\u2705 Detection completed!\" &lt;&lt; std::endl;\n            std::cout &lt;&lt; \"\ud83d\udcca Found \" &lt;&lt; detections.size() &lt;&lt; \" objects\" &lt;&lt; std::endl;\n\n            for (size_t i = 0; i &lt; detections.size(); ++i) {\n                std::cout &lt;&lt; \"   [\" &lt;&lt; i &lt;&lt; \"] Class=\" &lt;&lt; detections[i].classId \n                          &lt;&lt; \", Confidence=\" &lt;&lt; std::fixed &lt;&lt; std::setprecision(2) &lt;&lt; detections[i].conf\n                          &lt;&lt; \", Box=(\" &lt;&lt; detections[i].box.x &lt;&lt; \",\" &lt;&lt; detections[i].box.y &lt;&lt; \",\"\n                          &lt;&lt; detections[i].box.width &lt;&lt; \"x\" &lt;&lt; detections[i].box.height &lt;&lt; \")\" &lt;&lt; std::endl;\n            }\n\n            // Draw detections\n            cv::Mat resultImage = image.clone();\n            detector.drawDetections(resultImage, detections);\n\n            // Save output with timestamp\n            std::string outputPath = utils::saveImage(resultImage, imgPath, outputDir);\n            std::cout &lt;&lt; \"\ud83d\udcbe Saved result to: \" &lt;&lt; outputPath &lt;&lt; std::endl;\n\n            // Display metrics\n            utils::printMetrics(\"Detection\", duration.count());\n\n            // Display result\n            cv::imshow(\"YOLO Detection\", resultImage);\n            std::cout &lt;&lt; \"Press any key to continue...\" &lt;&lt; std::endl;\n            cv::waitKey(0);\n        }\n\n        cv::destroyAllWindows();\n        std::cout &lt;&lt; \"\\n\u2705 All images processed successfully!\" &lt;&lt; std::endl;\n\n    } catch (const std::exception&amp; e) {\n        std::cerr &lt;&lt; \"\u274c Error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>YOLOs-CPP is licensed under the GNU Affero General Public License v3.0.</p>"},{"location":"#citation","title":"Citation","text":"<p>@software{YOLOs-CPP2026,   author = {YOLOs-CPP contributors},   title = {YOLOs-CPP: Modern C++ YOLO Library},   year = {2026},   url = {https://github.com/Geekgineer/YOLOs-CPP},   license = {AGPL-3.0} }</p>"},{"location":"YOLOs-CPP_on_Windows_11/","title":"YOLOs-CPP","text":"<p>YOLOs-CPP is a C++ implementation for running YOLO models (v5, v7, v8, v9, v10, v11, v12) for object detection, instance segmentation, oriented bounding boxes (OBB), pose estimation, and quantized model support. The project uses ONNX Runtime and OpenCV for efficient inference on images, videos, and camera feeds. All setup, model export, quantization, and testing were performed by the project owner on Windows 11 (x64).</p>"},{"location":"YOLOs-CPP_on_Windows_11/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Prerequisites</li> <li>Installation</li> <li>Windows-Specific Instructions</li> <li>Building the Project</li> <li>Usage Instructions</li> <li>Python Scripts for Model Export</li> <li>Modifications Made</li> <li>Troubleshooting</li> <li>Notes</li> </ul>"},{"location":"YOLOs-CPP_on_Windows_11/#overview","title":"Overview","text":"<p>This project provides a C++ framework for running YOLO models for object detection, instance segmentation, oriented bounding boxes (OBB), and pose estimation. It supports: - Real-time inference on camera feeds (<code>camera_inference.cpp</code>). - Processing static images (<code>image_inference.cpp</code>). - Processing video files (<code>video_inference.cpp</code>). - Quantized models for faster CPU inference (<code>yolo11n_uint8.onnx</code>).</p> <p>All steps, from installation to testing, were executed and verified on Windows 11 (x64) using Visual Studio 2022, CMake, OpenCV 4.6.0, and ONNX Runtime.</p>"},{"location":"YOLOs-CPP_on_Windows_11/#prerequisites","title":"Prerequisites","text":"<ul> <li>C++17 compatible compiler</li> <li>CMake (version 3.10 or higher)</li> <li>OpenCV (version 4.5.5 or higher)</li> <li>ONNX Runtime (version 1.16.3 or 1.19.2 recommended, with optional GPU support)</li> <li>Python Version 3.8+ with <code>onnxruntime</code> and <code>ultralytics</code> packages.</li> <li>Git for cloning the repository</li> </ul>"},{"location":"YOLOs-CPP_on_Windows_11/#installation","title":"Installation","text":""},{"location":"YOLOs-CPP_on_Windows_11/#windows-specific-instructions","title":"Windows-Specific Instructions","text":"<p>All steps were performed by the project owner on Windows 11 (x64).</p> <ol> <li>Install Visual Studio 2022:</li> <li>Open https://visualstudio.microsoft.com/vs/.</li> <li>Click \"Free download\" under \"Community 2022\".</li> <li>Run the installer and select Desktop development with C++.</li> <li>Ensure MSVC v143 - VS 2022 C++ x64/x86 build tools is checked in the right panel.</li> <li>Click Install and restart your system if prompted.</li> <li> <p>Verify by opening Visual Studio and creating a new C++ project.</p> </li> <li> <p>Install CMake:</p> </li> <li>Open https://cmake.org/download/.</li> <li>Download the latest Windows x64 Installer.</li> <li>Run the installer and select Add CMake to the system PATH for all users.</li> <li> <p>Verify by opening PowerShell or CMD and running:      <pre><code>cmake --version\n</code></pre></p> <ul> <li>Expected output: <code>cmake version 3.x.x</code>.</li> </ul> </li> <li> <p>Install Python 3.8+:</p> </li> <li>Open https://www.python.org/downloads/.</li> <li>Download the latest Python 3.8+ installer.</li> <li>Run the installer and check Add Python to PATH in the first screen.</li> <li>Install and verify by running:      <pre><code>python --version\npip --version\n</code></pre></li> <li>Install required packages:      <pre><code>pip install onnx onnxruntime numpy ultralytics\n</code></pre></li> <li>If you see \"Python was not found\":<ul> <li>Open Start, search for \"App execution aliases\".</li> <li>Turn off <code>python.exe</code> and <code>python3.exe</code>.</li> </ul> </li> <li> <p>Verify Python packages:      <pre><code>python -c \"import cv2; print(cv2.__version__)\"\n</code></pre></p> <ul> <li>Expected output: <code>4.6.0</code> or higher.</li> </ul> </li> <li> <p>Install Git:</p> </li> <li>Open https://git-scm.com/download/win.</li> <li>Download and run the installer.</li> <li>Verify by running:      <pre><code>git --version\n</code></pre><ul> <li>Expected output: <code>git version 2.x.x</code>.</li> </ul> </li> </ol>"},{"location":"YOLOs-CPP_on_Windows_11/#set-up-dlls","title":"Set Up DLLs","text":"<ol> <li>Install OpenCV:</li> <li>Open https://opencv.org/releases/.</li> <li>Download OpenCV 4.6.0 for Windows.</li> <li>Extract to <code>C:\\opencv</code>.</li> <li>Copy the following DLLs from <code>C:\\opencv\\build\\x64\\vc16\\bin</code> to <code>C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\build\\Release</code> (after building):<ul> <li><code>opencv_videoio_ffmpeg460_64.dll</code></li> <li><code>opencv_videoio_msmf460_64.dll</code></li> <li><code>opencv_world460.dll</code></li> </ul> </li> <li>Add OpenCV\u2019s <code>bin</code> directory to the system PATH:<ul> <li>Open Start, search for \"Edit the system environment variables\".</li> <li>Click Environment Variables.</li> <li>Under System variables, select Path, click Edit.</li> <li>Click New and add:    <pre><code>C:\\opencv\\build\\x64\\vc16\\bin\n</code></pre></li> <li>Click OK to save all changes.</li> </ul> </li> <li>Restart PowerShell or CMD.</li> <li> <p>Verify:      <pre><code>python -c \"import cv2; print(cv2.__version__)\"\n</code></pre></p> <ul> <li>Expected output: <code>4.6.0</code>.</li> </ul> </li> <li> <p>Install ONNX Runtime:</p> </li> <li>Open https://github.com/microsoft/onnxruntime/releases.</li> <li>Download the Windows x64 ZIP (e.g., <code>onnxruntime-win-x64-gpu-1.16.3.zip</code> for GPU support).</li> <li>Extract to <code>C:\\onnxruntime</code>.</li> <li>Copy the following DLLs from <code>C:\\onnxruntime\\lib</code> to <code>C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\build\\Release</code> (after building):<ul> <li><code>onnxruntime.dll</code></li> <li><code>onnxruntime_providers_shared.dll</code></li> <li><code>onnxruntime_providers_cuda.dll</code> (if using GPU)</li> <li><code>onnxruntime_providers_tensorrt.dll</code> (if using GPU)</li> </ul> </li> <li>Add ONNX Runtime\u2019s <code>lib</code> directory to the system PATH:<ul> <li>Open Environment Variables as above.</li> <li>Select Path, click Edit.</li> <li>Click New and add:    <pre><code>C:\\onnxruntime\\lib\n</code></pre></li> <li>Click OK to save.</li> </ul> </li> <li>Restart PowerShell or CMD.</li> </ol>"},{"location":"YOLOs-CPP_on_Windows_11/#clone-the-repository","title":"Clone the Repository","text":"<ul> <li>Open PowerShell or CMD.</li> <li>Run:      <pre><code>cd C:\\Users\\DELL\\source\\repos\ngit clone https://github.com/Geekgineer/YOLOs-CPP.git\ncd YOLOs-CPP\n</code></pre></li> </ul>"},{"location":"YOLOs-CPP_on_Windows_11/#export-yolo-models-to-onnx","title":"Export YOLO Models to ONNX","text":"<ol> <li>Navigate to the <code>models</code> directory:    <pre><code>cd C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\models\n</code></pre></li> <li>Run the export scripts:</li> <li>Object Detection:      <pre><code>python export_onnx.py\n</code></pre><ul> <li>Exports <code>yolo11n.pt</code> to <code>yolo11n.onnx</code>.</li> </ul> </li> <li>Segmentation:      <pre><code>python export_onnx_to_segment.py\n</code></pre><ul> <li>Exports <code>yolo11n-seg.pt</code> to <code>yolo11n-seg.onnx</code>.</li> </ul> </li> <li>OBB:      <pre><code>python export_onnx_to_obb.py\n</code></pre><ul> <li>Exports <code>yolo11n-obb.pt</code> to <code>yolo11n-obb.onnx</code>.</li> </ul> </li> <li>Pose:      <pre><code>python export_onnx_to_pose.py\n</code></pre><ul> <li>Exports <code>yolo11n-pose.pt</code> to <code>yolo11n-pose.onnx</code>.</li> </ul> </li> <li>Verify that the ONNX files are in <code>C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\models</code>.</li> </ol>"},{"location":"YOLOs-CPP_on_Windows_11/#quantize-yolo-model","title":"Quantize YOLO Model","text":"<ol> <li>Navigate to the <code>quantized_models</code> directory:    <pre><code>cd C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\quantized_models\n</code></pre></li> <li>Run the quantization script:    <pre><code>python yolos_quantization.py\n</code></pre></li> <li>Quantizes <code>yolo11n.onnx</code> to <code>yolo11n_uint8.onnx</code> using per-channel quantization (UINT8).</li> <li>Verify that <code>yolo11n_uint8.onnx</code> is in <code>C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\models</code>.</li> </ol>"},{"location":"YOLOs-CPP_on_Windows_11/#building-the-project","title":"Building the Project","text":"<ol> <li>Navigate to the project directory:    <pre><code>cd C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\n</code></pre></li> <li>Create and navigate to the build directory:    <pre><code>mkdir build\ncd build\n</code></pre></li> <li> <p>Configure the project with CMake:    <pre><code>cmake .. -G \"Visual Studio 17 2022\" -A x64 -DOpenCV_DIR=\"C:/opencv/build\" -DONNXRUNTIME_DIR=\"C:/onnxruntime\"\n</code></pre></p> </li> <li> <p>Build the project in Release mode:    <pre><code>cmake --build . --config Release\n</code></pre></p> </li> </ol>"},{"location":"YOLOs-CPP_on_Windows_11/#usage-instructions","title":"Usage Instructions","text":"<p>All commands were tested successfully by the project owner on Windows 11 (x64). Ensure model files, label files, and input files are in the specified paths.</p>"},{"location":"YOLOs-CPP_on_Windows_11/#object-detection-quantized-model","title":"Object Detection (Quantized Model)","text":"<p>Uses <code>yolo11n_uint8.onnx</code> for faster CPU inference. - Image Inference:   <pre><code>cd C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\build\n.\\Release\\image_inference.exe C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\data\\dog.jpg\n</code></pre>   - Output: <code>output_dog.jpg</code> with bounding boxes. - Video Inference:   <pre><code>.\\Release\\video_inference.exe C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\data\\test.mp4\n</code></pre>   - Output: <code>output_video.mp4</code> with bounding boxes. - Camera Inference:   <pre><code>.\\Release\\camera_inference.exe\n</code></pre>   - Output: PNG frames in <code>C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\build\\Release\\output_frames</code>.</p>"},{"location":"YOLOs-CPP_on_Windows_11/#instance-segmentation","title":"Instance Segmentation","text":"<p>Uses <code>yolo11n-seg.onnx</code> with <code>YOLO11SegDetector</code> (<code>seg/YOLO11Seg.hpp</code>). - Image Inference:   <pre><code>.\\Release\\image_inference.exe C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\data\\dog.jpg\n</code></pre>   - Output: <code>output_dog.jpg</code> with segmentation masks. - Video Inference:   <pre><code>.\\Release\\video_inference.exe C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\data\\test.mp4\n</code></pre>   - Output: <code>output_video.mp4</code> with segmentation masks. - Camera Inference:   <pre><code>.\\Release\\camera_inference.exe\n</code></pre>   - Output: PNG frames in <code>C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\build\\Release\\output_frames</code>.</p>"},{"location":"YOLOs-CPP_on_Windows_11/#oriented-bounding-boxes-obb","title":"Oriented Bounding Boxes (OBB)","text":"<p>Uses <code>yolo11n-obb.onnx</code> with <code>YOLO11OBBDetector</code> (<code>obb/YOLO11-OBB.hpp</code>) and <code>Dota.names</code>. - Image Inference:   <pre><code>.\\Release\\image_inference.exe C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\data\\dog.jpg\n</code></pre>   - Output: <code>output_dog_obb.jpg</code> with oriented bounding boxes. - Video Inference:   <pre><code>.\\Release\\video_inference.exe C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\data\\test.mp4\n</code></pre>   - Output: <code>output_video_obb.mp4</code> with oriented bounding boxes. - Camera Inference:   <pre><code>.\\Release\\camera_inference.exe\n</code></pre>   - Output: PNG frames in <code>C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\build\\Release\\output_frames_obb</code>.</p>"},{"location":"YOLOs-CPP_on_Windows_11/#pose-estimation","title":"Pose Estimation","text":"<p>Uses <code>yolo11n-pose.onnx</code> with <code>YOLO11POSEDetector</code> (<code>pose/YOLO11-POSE.hpp</code>) on inputs with persons. - Image Inference:   <pre><code>.\\Release\\image_inference.exe C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\data\\person.jpg\n</code></pre>   - Output: <code>output_pose.jpg</code> with keypoints and skeletons. - Video Inference:   <pre><code>.\\Release\\video_inference.exe C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\data\\test_pose.mp4\n</code></pre>   - Output: <code>output_video_pose.mp4</code> with keypoints and skeletons. - Camera Inference:   <pre><code>.\\Release\\camera_inference.exe\n</code></pre>   - Output: PNG frames in <code>C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\build\\Release\\output_frames_pose</code>.</p>"},{"location":"YOLOs-CPP_on_Windows_11/#python-scripts-for-model-export","title":"Python Scripts for Model Export","text":"<p>The following Python scripts were created or modified by the project owner to export and quantize YOLO models.</p> <ol> <li> <p><code>export_onnx.py</code> (modified):    <pre><code>from ultralytics import YOLO\n\n# Load the YOLOv11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export the model to ONNX format\nmodel.export(format=\"onnx\")\n</code></pre></p> </li> <li> <p><code>export_onnx_to_segment.py</code> (created):    <pre><code>from ultralytics import YOLO\n\n# Load the YOLOv11 segmentation model\nmodel = YOLO(\"yolo11n-seg.pt\")\n\n# Export to ONNX format\nmodel.export(format=\"onnx\", task=\"segment\")\n</code></pre></p> </li> <li> <p><code>export_onnx_to_obb.py</code> (created):    <pre><code>from ultralytics import YOLO\n\n# Load the YOLOv11 OBB model\nmodel = YOLO(\"yolo11n-obb.pt\")\n\n# Export to ONNX format\nmodel.export(format=\"onnx\", task=\"obb\")\n</code></pre></p> </li> <li> <p><code>export_onnx_to_pose.py</code> (created):    <pre><code>from ultralytics import YOLO\n\n# Load the YOLOv11 pose model\nmodel = YOLO(\"yolo11n-pose.pt\")\n\n# Export to ONNX format\nmodel.export(format=\"onnx\", task=\"pose\")\n</code></pre></p> </li> <li> <p><code>yolos_quantization.py</code> (modified):    <pre><code>from onnxruntime.quantization import quantize_dynamic, QuantType\nfrom pathlib import Path\nfrom typing import Union\n\ndef quantize_onnx_model(onnx_model_path: Union[str, Path], quantized_model_path: Union[str, Path], per_channel: bool = False):\n    \"\"\"\n    Quantizes an ONNX model and saves the quantized version.\n\n    Args:\n        onnx_model_path: Path to the original ONNX model file.\n        quantized_model_path: Path to save the quantized model.\n        per_channel: If True, quantizes weights per channel instead of per layer.\n            Per-channel quantization can improve model accuracy by allowing each output channel\n            to have its own scale and zero-point, which better captures the distribution of weights.\n            This is especially beneficial for complex models with many channels or varying value ranges.\n            Use this option when:\n            - The model is complex (e.g., deep convolutional networks).\n            - You observe accuracy degradation with per-layer quantization.\n    \"\"\"\n    # Quantize the model\n    quantize_dynamic(\n        model_input=onnx_model_path,\n        model_output=quantized_model_path,\n        per_channel=per_channel,\n        weight_type=QuantType.QUInt8\n    )\n\n    print(\"Quantization completed. Quantized model saved to:\", quantized_model_path)\n\nif __name__ == \"__main__\":\n    # Load the original ONNX model file path\n    onnx_model_path = 'C:/Users/DELL/source/repos/YOLOs-CPP/models/yolo11n.onnx'\n    # Specify the output path for the quantized model\n    quantized_model_path = 'C:/Users/DELL/source/repos/YOLOs-CPP/models/yolo11n_uint8.onnx'\n    # Call the quantization function\n    quantize_onnx_model(onnx_model_path, quantized_model_path, per_channel=True)\n</code></pre></p> </li> </ol> <p>6- <code>CMakeLists.txt</code> (modified):    ```txt    cmake_minimum_required(VERSION 3.0.0)    project(yolo_ort)</p> <p>option(ONNXRUNTIME_DIR \"Path to built ONNX Runtime directory.\" STRING)    message(STATUS \"ONNXRUNTIME_DIR: ${ONNXRUNTIME_DIR}\")</p> <p>find_package(OpenCV REQUIRED)</p> <p>include_directories(\"include/\")</p> <p># Add executable for image inference    add_executable(image_inference                   src/image_inference.cpp)</p> <p># Add executable for camera inference    add_executable(camera_inference                   src/camera_inference.cpp)</p> <p># Add executable for video inference    add_executable(video_inference                   src/video_inference.cpp)</p> <p>set(CMAKE_CXX_STANDARD 17)    set(CMAKE_CXX_STANDARD_REQUIRED ON)</p> <p># Set include directories for all executables    target_include_directories(image_inference PRIVATE \"\\({ONNXRUNTIME_DIR}/include\")    target_include_directories(camera_inference PRIVATE \"\\)/include\")    target_include_directories(video_inference PRIVATE \"${ONNXRUNTIME_DIR}/include\")</p> <p># Set compile features for all executables    target_compile_features(image_inference PRIVATE cxx_std_17)    target_compile_features(camera_inference PRIVATE cxx_std_17)    target_compile_features(video_inference PRIVATE cxx_std_17)</p> <p># Link libraries for all executables     #### Replace ${OpenCV_LIBS} with opencv_world    target_link_libraries(image_inference opencv_world)    target_link_libraries(camera_inference opencv_world)    target_link_libraries(video_inference opencv_world)</p> <p>if(UNIX)        message(STATUS \"We are building on Linux!\")        # Specific Linux build commands or flags        target_link_libraries(image_inference \"\\({ONNXRUNTIME_DIR}/lib/libonnxruntime.so\")        target_link_libraries(camera_inference \"\\)/lib/libonnxruntime.so\")        target_link_libraries(video_inference \"${ONNXRUNTIME_DIR}/lib/libonnxruntime.so\")    endif(UNIX)</p> <p>if(APPLE)        message(STATUS \"We are building on macOS!\")        # Specific macOS build commands or flags        target_link_libraries(image_inference \"\\({ONNXRUNTIME_DIR}/lib/libonnxruntime.dylib\")        target_link_libraries(camera_inference \"\\)/lib/libonnxruntime.dylib\")        target_link_libraries(video_inference \"${ONNXRUNTIME_DIR}/lib/libonnxruntime.dylib\")    endif(APPLE)</p> <p>if(WIN32)        message(STATUS \"We are building on Windows!\")        # Specific Windows build commands or flags        target_link_libraries(image_inference \"\\({ONNXRUNTIME_DIR}/lib/onnxruntime.lib\")        target_link_libraries(camera_inference \"\\)/lib/onnxruntime.lib\")        target_link_libraries(video_inference \"${ONNXRUNTIME_DIR}/lib/onnxruntime.lib\")    endif(WIN32) ```</p>"},{"location":"YOLOs-CPP_on_Windows_11/#modifications-made","title":"Modifications Made","text":"<p>The project owner made the following changes: - Modified <code>yolos_quantization.py</code> to support <code>yolo11n.onnx</code> and generate <code>yolo11n_uint8.onnx</code>. - Modified <code>export_onnx.py</code> to export <code>yolo11n.pt</code> to <code>yolo11n.onnx</code>. - Created <code>export_onnx_to_segment.py</code>, <code>export_onnx_to_obb.py</code>, and <code>export_onnx_to_pose.py</code> to export segmentation, OBB, and pose models. - Modified <code>camera_inference.cpp</code>, <code>image_inference.cpp</code>, and <code>video_inference.cpp</code> to support:   - Quantized model (<code>yolo11n_uint8.onnx</code>) with <code>YOLO11Detector</code>.   - Segmentation (<code>yolo11n-seg.onnx</code>) with <code>YOLO11SegDetector</code> and <code>drawSegmentations</code>.   - OBB (<code>yolo11n-obb.onnx</code>) with <code>YOLO11OBBDetector</code> and <code>Dota.names</code>.   - Pose (<code>yolo11n-pose.onnx</code>) with <code>YOLO11POSEDetector</code> and keypoints/skeletons drawing. - Updated paths in all <code>.cpp</code> files to use absolute paths (e.g., <code>C:/Users/DELL/source/repos/YOLOs-CPP/models</code>). - Set <code>isGPU = false</code> for CPU processing. - Added file existence checks using <code>fs::exists</code>. - Added detailed logging for detection time and details. - Added saving of outputs (PNG frames for camera, images for image inference, videos for video inference). - Tested all models and confirmed successful results with bounding boxes, masks, oriented boxes, and keypoints/skeletons.</p>"},{"location":"YOLOs-CPP_on_Windows_11/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Model file does not exist:</li> <li>Verify that <code>yolo11n_uint8.onnx</code>, <code>yolo11n-seg.onnx</code>, <code>yolo11n-obb.onnx</code>, and <code>yolo11n-pose.onnx</code> are in <code>C:/Users/DELL/source/repos/YOLOs-CPP/models</code>.</li> <li>Run the export scripts (<code>export_onnx.py</code>, <code>export_onnx_to_segment.py</code>, etc.) to generate missing models.</li> <li>Labels file does not exist:</li> <li>Ensure <code>coco.names</code> (for detection, segmentation, pose) and <code>Dota.names</code> (for OBB) are in <code>C:/Users/DELL/source/repos/YOLOs-CPP/quantized_models</code>.</li> <li>Could not open video/image:</li> <li>Check that <code>dog.jpg</code>, <code>person.jpg</code>, <code>test.mp4</code>, and <code>test_pose.mp4</code> are in <code>C:/Users/DELL/source/repos/YOLOs-CPP/data</code>.</li> <li>No poses detected:</li> <li>Use inputs with persons (e.g., <code>person.jpg</code>, <code>test_pose.mp4</code>) for pose estimation.</li> <li>DLL errors:</li> <li>Ensure all DLLs (<code>opencv_*.dll</code>, <code>onnxruntime*.dll</code>) are in <code>C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\build\\Release</code>.</li> <li>Verify that <code>C:\\opencv\\build\\x64\\vc16\\bin</code> and <code>C:\\onnxruntime\\lib</code> are in the system PATH.</li> <li>CMake errors:</li> <li>Ensure <code>OpenCV_DIR</code> and <code>ONNXRUNTIME_DIR</code> are correctly set in the CMake command.</li> <li>If <code>std::filesystem</code> errors occur, update <code>CMakeLists.txt</code> to use C++17 (see Building the Project).</li> <li>Add include directories in <code>CMakeLists.txt</code> for segmentation, OBB, and pose:     <pre><code>include_directories(${CMAKE_SOURCE_DIR}/seg)\ninclude_directories(${CMAKE_SOURCE_DIR}/obb)\ninclude_directories(${CMAKE_SOURCE_DIR}/pose)\n</code></pre></li> </ul>"},{"location":"YOLOs-CPP_on_Windows_11/#notes","title":"Notes","text":"<ul> <li>All models were tested on CPU (<code>isGPU = false</code>) for compatibility. Enable GPU by setting <code>isGPU = true</code> if supported hardware is available (include <code>onnxruntime_providers_cuda.dll</code> and <code>onnxruntime_providers_tensorrt.dll</code>).</li> <li>Quantized models (<code>yolo11n_uint8.onnx</code>) provide faster inference with minimal accuracy loss.</li> <li>OBB requires <code>Dota.names</code> for correct class labeling.</li> <li>Pose estimation requires inputs with persons to detect keypoints and skeletons.</li> <li>Outputs are saved as:</li> <li>Images: <code>output_&lt;filename&gt;.jpg</code> (e.g., <code>output_dog.jpg</code>, <code>output_pose.jpg</code>).</li> <li>Videos: <code>output_video.mp4</code>, <code>output_video_obb.mp4</code>, <code>output_video_pose.mp4</code>.</li> <li>Camera frames: PNGs in <code>output_frames</code>, <code>output_frames_obb</code>, or <code>output_frames_pose</code>.</li> </ul>"},{"location":"YOLOs-CPP_on_Windows_11/#if-you-encounter-errors-related-to-stdfilesystem-requiring-c17","title":"If you encounter errors related to <code>std::filesystem</code> (requiring C++17):","text":"<ul> <li>Open <code>CMakeLists.txt</code> in the project root (<code>C:\\Users\\DELL\\source\\repos\\YOLOs-CPP</code>).</li> <li>Replace like in \"CMakeLists.txt (modified)\":</li> <li>Save the file.</li> <li>Delete the <code>build</code> directory and recreate it:     <pre><code>cd C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\nRemove-Item -Recurse -Force build\nmkdir build\ncd build\ncmake .. -G \"Visual Studio 17 2022\" -A x64 -DOpenCV_DIR=\"C:/opencv/build\" -DONNXRUNTIME_DIR=\"C:/onnxruntime\"\n</code></pre></li> </ul>"},{"location":"YOLOs-CPP_on_Windows_11/#if-there-error-in-path","title":"if there error in path","text":"<ul> <li>Copy DLLs to <code>build\\Release</code>:</li> <li>From <code>C:\\opencv\\build\\x64\\vc16\\bin</code>:<ul> <li><code>opencv_videoio_ffmpeg460_64.dll</code></li> <li><code>opencv_videoio_msmf460_64.dll</code></li> <li><code>opencv_world460.dll</code></li> </ul> </li> <li>From <code>C:\\onnxruntime\\lib</code>:<ul> <li><code>onnxruntime.dll</code></li> <li><code>onnxruntime_providers_shared.dll</code></li> <li><code>onnxruntime_providers_cuda.dll</code> (if using GPU)</li> <li><code>onnxruntime_providers_tensorrt.dll</code> (if using GPU)</li> </ul> </li> <li>Paste all DLLs into <code>C:\\Users\\DELL\\source\\repos\\YOLOs-CPP\\build\\Release</code>.</li> </ul>"},{"location":"acknowledgments/","title":"Acknowledgments","text":"<p>YOLOs-CPP is built on the foundation of outstanding open-source projects.</p>"},{"location":"acknowledgments/#core-dependencies","title":"Core Dependencies","text":"Project Purpose License Ultralytics YOLO models &amp; training AGPL-3.0 ONNX Runtime High-performance inference MIT OpenCV Computer vision primitives Apache-2.0"},{"location":"acknowledgments/#inspiration","title":"Inspiration","text":"<p>This project was inspired by and builds upon work from:</p> <ul> <li>itsnine/yolov5-onnxruntime</li> <li>WongKinYiu/yolov7</li> <li>Li-99/yolov8_onnxruntime</li> </ul>"},{"location":"acknowledgments/#contributors","title":"Contributors","text":"<p>Thanks to everyone who has contributed to YOLOs-CPP:</p> <p> </p>"},{"location":"acknowledgments/#special-thanks","title":"Special Thanks","text":"<ul> <li>The Ultralytics team for their continuous improvements to YOLO</li> <li>Microsoft for the ONNX Runtime project</li> <li>The OpenCV community for their computer vision library</li> <li>All our users and contributors</li> </ul> <p>If you'd like to contribute, see our Contributing Guide.</p>"},{"location":"contributing/","title":"Contributing to YOLOs-CPP","text":"<p>Thank you for your interest in contributing! This guide will help you get started.</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<ul> <li>Bug reports \u2014 Found an issue? Open a GitHub issue</li> <li>Feature requests \u2014 Have an idea? Let's discuss it</li> <li>Code contributions \u2014 Fix bugs or add features</li> <li>Documentation \u2014 Improve docs, add examples</li> <li>Testing \u2014 Add test cases, improve coverage</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code>git clone https://github.com/YOUR_USERNAME/YOLOs-CPP.git\ncd YOLOs-CPP\n</code></pre>"},{"location":"contributing/#2-create-a-branch","title":"2. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/issue-description\n</code></pre>"},{"location":"contributing/#3-build-and-test","title":"3. Build and Test","text":"<pre><code># Build\n./build.sh 1.20.1 0\n\n# Run tests\ncd tests &amp;&amp; ./test_all.sh\n</code></pre>"},{"location":"contributing/#code-guidelines","title":"Code Guidelines","text":""},{"location":"contributing/#style","title":"Style","text":"<ul> <li>C++17 standard</li> <li>4 spaces for indentation (no tabs)</li> <li>snake_case for variables and functions</li> <li>PascalCase for classes and types</li> <li>Max line length: 100 characters</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Document public APIs with <code>///</code> comments</li> <li>Include <code>@brief</code>, <code>@param</code>, <code>@return</code> for functions</li> <li>Add code examples where helpful</li> </ul>"},{"location":"contributing/#example","title":"Example","text":"<pre><code>/// @brief Detect objects in an image\n/// @param image Input image (BGR format)\n/// @param confThreshold Confidence threshold [0, 1]\n/// @param iouThreshold IoU threshold for NMS [0, 1]\n/// @return Vector of detections\n[[nodiscard]] std::vector&lt;Detection&gt; detect(\n    const cv::Mat&amp; image,\n    float confThreshold = 0.25f,\n    float iouThreshold = 0.45f\n);\n</code></pre>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>Use conventional commits:</p> <pre><code>feat: add YOLO-World support\nfix: correct NMS threshold handling\ndocs: update installation guide\ntest: add segmentation edge cases\nrefactor: simplify preprocessing pipeline\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#1-before-submitting","title":"1. Before Submitting","text":"<ul> <li> Code compiles without warnings</li> <li> All tests pass (<code>./test_all.sh</code>)</li> <li> New code has tests</li> <li> Documentation is updated</li> </ul>"},{"location":"contributing/#2-submit-pr","title":"2. Submit PR","text":"<ul> <li>Open PR against <code>main</code> branch</li> <li>Fill out the PR template</li> <li>Link related issues</li> </ul>"},{"location":"contributing/#3-review-process","title":"3. Review Process","text":"<ul> <li>Maintainers will review within 3-5 days</li> <li>Address feedback promptly</li> <li>Once approved, we'll merge</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":""},{"location":"contributing/#run-all-tests","title":"Run All Tests","text":"<pre><code>cd tests\n./test_all.sh\n</code></pre>"},{"location":"contributing/#run-specific-tests","title":"Run Specific Tests","text":"<pre><code>./test_detection.sh\n./test_segmentation.sh\n./test_pose.sh\n./test_obb.sh\n./test_classification.sh\n</code></pre>"},{"location":"contributing/#add-new-tests","title":"Add New Tests","text":"<ol> <li>Add model to <code>tests/&lt;task&gt;/models/</code></li> <li>Update inference script</li> <li>Add comparison cases</li> </ol>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>Include: - YOLOs-CPP version - OS and compiler version - ONNX Runtime version - Minimal reproduction steps - Error messages / logs</p>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>Include: - Use case description - Expected behavior - Any relevant examples</p>"},{"location":"contributing/#community","title":"Community","text":"<ul> <li>GitHub Issues \u2014 Bug reports, feature requests</li> <li>GitHub Discussions \u2014 Questions, ideas</li> <li>Pull Requests \u2014 Code contributions</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under AGPL-3.0.</p> <p>Thank you for helping make YOLOs-CPP better!</p>"},{"location":"development/","title":"Development Guide","text":"<p>Architecture overview, extending YOLOs-CPP, and debugging.</p>"},{"location":"development/#architecture","title":"Architecture","text":"<pre><code>include/yolos/\n\u251c\u2500\u2500 core/                    # Shared utilities\n\u2502   \u251c\u2500\u2500 types.hpp           # Detection, Segmentation types\n\u2502   \u251c\u2500\u2500 preprocessing.hpp   # Letterbox, normalization\n\u2502   \u251c\u2500\u2500 nms.hpp             # Non-maximum suppression\n\u2502   \u251c\u2500\u2500 drawing.hpp         # Visualization\n\u2502   \u251c\u2500\u2500 version.hpp         # YOLO version detection\n\u2502   \u251c\u2500\u2500 utils.hpp           # Helper functions\n\u2502   \u2514\u2500\u2500 session_base.hpp    # ONNX session wrapper\n\u251c\u2500\u2500 tasks/                   # Task implementations\n\u2502   \u251c\u2500\u2500 detection.hpp       # YOLODetector\n\u2502   \u251c\u2500\u2500 segmentation.hpp    # YOLOSegDetector\n\u2502   \u251c\u2500\u2500 pose.hpp            # YOLOPoseDetector\n\u2502   \u251c\u2500\u2500 obb.hpp             # YOLOOBBDetector\n\u2502   \u2514\u2500\u2500 classification.hpp  # YOLOClassifier\n\u2514\u2500\u2500 yolos.hpp               # Master include\n</code></pre>"},{"location":"development/#core-components","title":"Core Components","text":""},{"location":"development/#preprocessing-preprocessinghpp","title":"Preprocessing (<code>preprocessing.hpp</code>)","text":"<pre><code>// Letterbox with padding\ncv::Mat blob = yolos::preprocessing::letterBoxToBlob(\n    image,\n    cv::Size(640, 640),\n    scalePad  // Returns scale and padding info\n);\n</code></pre>"},{"location":"development/#nms-nmshpp","title":"NMS (<code>nms.hpp</code>)","text":"<pre><code>// Class-aware batched NMS\nstd::vector&lt;int&gt; indices;\nyolos::nms::NMSBoxesFBatched(\n    boxes, scores, classIds,\n    confThreshold, iouThreshold,\n    indices\n);\n</code></pre>"},{"location":"development/#drawing-drawinghpp","title":"Drawing (<code>drawing.hpp</code>)","text":"<pre><code>yolos::drawing::drawBoundingBox(image, box, label, color);\nyolos::drawing::drawMask(image, mask, color, alpha);\nyolos::drawing::drawKeypoints(image, keypoints);\n</code></pre>"},{"location":"development/#adding-a-new-yolo-version","title":"Adding a New YOLO Version","text":""},{"location":"development/#step-1-update-version-enum","title":"Step 1: Update Version Enum","text":"<pre><code>// include/yolos/core/version.hpp\nenum class YOLOVersion {\n    V5, V6, V7, V8, V9, V10, V11, V12, V26,\n    VNew  // Add your version\n};\n</code></pre>"},{"location":"development/#step-2-implement-postprocessing","title":"Step 2: Implement Postprocessing","text":"<pre><code>// include/yolos/tasks/detection.hpp\nvoid postprocessVNew(/* params */) {\n    // Parse model output\n    // Apply NMS\n    // Return detections\n}\n</code></pre>"},{"location":"development/#step-3-update-factory","title":"Step 3: Update Factory","text":"<pre><code>switch (version) {\n    case YOLOVersion::VNew:\n        return postprocessVNew(...);\n    // ...\n}\n</code></pre>"},{"location":"development/#step-4-add-tests","title":"Step 4: Add Tests","text":"<pre><code>// tests/detection/compare_results.cpp\n// Add model to test suite\n</code></pre>"},{"location":"development/#onnx-runtime-integration","title":"ONNX Runtime Integration","text":""},{"location":"development/#session-management","title":"Session Management","text":"<pre><code>Ort::Env env(ORT_LOGGING_LEVEL_WARNING, \"YOLOs-CPP\");\nOrt::SessionOptions options;\n\n// CPU optimization\noptions.SetIntraOpNumThreads(4);\noptions.SetGraphOptimizationLevel(ORT_ENABLE_ALL);\n\n// GPU (CUDA)\nOrtCUDAProviderOptions cuda_options{};\noptions.AppendExecutionProvider_CUDA(cuda_options);\n\nOrt::Session session(env, \"model.onnx\", options);\n</code></pre>"},{"location":"development/#memory-efficiency","title":"Memory Efficiency","text":"<pre><code>// Pre-allocate buffers\nstd::vector&lt;float&gt; inputBuffer(3 * 640 * 640);\n\n// Create tensor from existing memory\nOrt::Value::CreateTensor&lt;float&gt;(\n    memoryInfo,\n    inputBuffer.data(),\n    inputBuffer.size(),\n    inputShape.data(),\n    inputShape.size()\n);\n</code></pre>"},{"location":"development/#debugging","title":"Debugging","text":""},{"location":"development/#enable-verbose-output","title":"Enable Verbose Output","text":"<pre><code>Ort::Env env(ORT_LOGGING_LEVEL_VERBOSE, \"Debug\");\n</code></pre>"},{"location":"development/#profile-inference","title":"Profile Inference","text":"<pre><code>#include &lt;chrono&gt;\n\nauto start = std::chrono::high_resolution_clock::now();\nauto detections = detector.detect(frame);\nauto end = std::chrono::high_resolution_clock::now();\n\nauto ms = std::chrono::duration&lt;double, std::milli&gt;(end - start).count();\nstd::cout &lt;&lt; \"Inference: \" &lt;&lt; ms &lt;&lt; \" ms\" &lt;&lt; std::endl;\n</code></pre>"},{"location":"development/#validate-against-python","title":"Validate Against Python","text":"<pre><code># Run comparison tests\ncd tests\n./test_detection.sh\n</code></pre>"},{"location":"development/#code-style","title":"Code Style","text":"<ul> <li>C++17 standard</li> <li>snake_case for variables and functions</li> <li>PascalCase for classes and types</li> <li>UPPER_CASE for constants</li> <li>Use <code>const</code> and <code>[[nodiscard]]</code> where appropriate</li> </ul>"},{"location":"development/#building-tests","title":"Building Tests","text":"<pre><code>cd tests\n./build_test.sh 0  # Detection\n./build_test.sh 1  # Classification\n./build_test.sh 2  # Segmentation\n./build_test.sh 3  # Pose\n./build_test.sh 4  # OBB\n</code></pre>"},{"location":"development/#benchmarking","title":"Benchmarking","text":"<pre><code>cd benchmarks\n./auto_bench.sh 1.20.1 0 yolo11n,yolov8n\n</code></pre>"},{"location":"development/#next-steps","title":"Next Steps","text":"<ul> <li>Contributing \u2014 Submit changes</li> <li>Model Guide \u2014 Model compatibility</li> </ul>"},{"location":"installation/","title":"Installation Guide","text":"<p>This guide covers system requirements, build options, and troubleshooting for YOLOs-CPP.</p>"},{"location":"installation/#system-requirements","title":"System Requirements","text":""},{"location":"installation/#minimum-requirements","title":"Minimum Requirements","text":"Component Requirement OS Linux (Ubuntu 20.04+), Windows 10+, macOS 12+ Compiler GCC 9+, Clang 10+, or MSVC 2019+ CMake 3.16 or higher OpenCV 4.5 or higher C++ Standard C++17"},{"location":"installation/#gpu-acceleration-optional","title":"GPU Acceleration (Optional)","text":"Component Requirement NVIDIA GPU Compute Capability 5.0+ CUDA Toolkit 11.0 or higher cuDNN 8.0+ (recommended)"},{"location":"installation/#quick-install","title":"Quick Install","text":""},{"location":"installation/#linux-macos","title":"Linux / macOS","text":"<pre><code># Clone the repository\ngit clone https://github.com/Geekgineer/YOLOs-CPP.git\ncd YOLOs-CPP\n\n# Build with auto-download of ONNX Runtime\n./build.sh 1.20.1 0   # CPU build\n./build.sh 1.20.1 1   # GPU build (requires CUDA)\n</code></pre>"},{"location":"installation/#windows","title":"Windows","text":"<p>See Windows Setup Guide for detailed Windows instructions.</p>"},{"location":"installation/#manual-build","title":"Manual Build","text":""},{"location":"installation/#step-1-install-dependencies","title":"Step 1: Install Dependencies","text":"<p>Ubuntu/Debian: <pre><code>sudo apt update\nsudo apt install -y build-essential cmake libopencv-dev\n</code></pre></p> <p>macOS (Homebrew): <pre><code>brew install cmake opencv\n</code></pre></p>"},{"location":"installation/#step-2-download-onnx-runtime","title":"Step 2: Download ONNX Runtime","text":"<pre><code># Linux x64 CPU\nwget https://github.com/microsoft/onnxruntime/releases/download/v1.20.1/onnxruntime-linux-x64-1.20.1.tgz\ntar -xzf onnxruntime-linux-x64-1.20.1.tgz\n\n# Linux x64 GPU\nwget https://github.com/microsoft/onnxruntime/releases/download/v1.20.1/onnxruntime-linux-x64-gpu-1.20.1.tgz\ntar -xzf onnxruntime-linux-x64-gpu-1.20.1.tgz\n</code></pre>"},{"location":"installation/#step-3-configure-and-build","title":"Step 3: Configure and Build","text":"<pre><code>mkdir build &amp;&amp; cd build\n\n# CPU build\ncmake .. \\\n  -DONNXRUNTIME_DIR=../onnxruntime-linux-x64-1.20.1 \\\n  -DCMAKE_BUILD_TYPE=Release\n\n# Compile\nmake -j$(nproc)\n</code></pre>"},{"location":"installation/#step-4-verify-installation","title":"Step 4: Verify Installation","text":"<pre><code>./image_inference ../models/yolo11n.onnx ../data/dog.jpg\n</code></pre>"},{"location":"installation/#build-options","title":"Build Options","text":"CMake Option Default Description <code>ONNXRUNTIME_DIR</code> auto-detect Path to ONNX Runtime installation <code>BUILD_EXAMPLES</code> OFF Build task-specific examples <code>CMAKE_BUILD_TYPE</code> Release Build type (Debug/Release)"},{"location":"installation/#docker-installation","title":"Docker Installation","text":"<pre><code># CPU\ndocker build -f Dockerfile.cpu -t yolos-cpp:cpu .\ndocker run --rm -it yolos-cpp:cpu\n\n# GPU (requires nvidia-docker)\ndocker build -t yolos-cpp:gpu .\ndocker run --gpus all --rm -it yolos-cpp:gpu\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#opencv-not-found","title":"\"OpenCV not found\"","text":"<pre><code>pkg-config --modversion opencv4\ncmake .. -DOpenCV_DIR=/path/to/opencv/build\n</code></pre>"},{"location":"installation/#onnx-runtime-not-found","title":"\"ONNX Runtime not found\"","text":"<pre><code>ls $ONNXRUNTIME_DIR/include/onnxruntime_cxx_api.h\n</code></pre>"},{"location":"installation/#build-fails-on-windows","title":"Build fails on Windows","text":"<p>See Windows Setup Guide.</p>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Usage Guide \u2014 Learn the API</li> <li>Model Guide \u2014 Export and use models</li> </ul>"},{"location":"installation/#windows-quick-start","title":"Windows Quick Start","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Visual Studio 2019+ with \"Desktop development with C++\"</li> <li>CMake 3.16+</li> <li>OpenCV 4.5+ (from opencv.org or vcpkg)</li> </ul>"},{"location":"installation/#option-1-powershell-script","title":"Option 1: PowerShell Script","text":"<pre><code># CPU build\n.\\build.ps1\n\n# GPU build (requires CUDA)\n.\\build.ps1 -GPU\n\n# Clean build\n.\\build.ps1 -Clean\n</code></pre>"},{"location":"installation/#option-2-batch-script","title":"Option 2: Batch Script","text":"<pre><code>build.bat          # CPU build\nbuild.bat gpu      # GPU build\n</code></pre>"},{"location":"installation/#option-3-manual-build","title":"Option 3: Manual Build","text":"<pre><code># Download ONNX Runtime\nInvoke-WebRequest -Uri \"https://github.com/microsoft/onnxruntime/releases/download/v1.20.1/onnxruntime-win-x64-1.20.1.zip\" -OutFile \"ort.zip\"\nExpand-Archive -Path \"ort.zip\" -DestinationPath \".\"\n\n# Build\nmkdir build; cd build\ncmake .. -DONNXRUNTIME_DIR=\"..\\onnxruntime-win-x64-1.20.1\"\ncmake --build . --config Release\n\n# Run\n.\\Release\\image_inference.exe ..\\models\\yolo11n.onnx ..\\data\\dog.jpg\n</code></pre>"},{"location":"installation/#setting-up-opencv-on-windows","title":"Setting up OpenCV on Windows","text":"<p>Option A: Pre-built binaries 1. Download from https://opencv.org/releases/ 2. Extract to <code>C:\\opencv</code> 3. Add <code>C:\\opencv\\build\\x64\\vc16\\bin</code> to PATH</p> <p>Option B: Using vcpkg <pre><code>vcpkg install opencv4:x64-windows\ncmake .. -DCMAKE_TOOLCHAIN_FILE=\"[vcpkg]/scripts/buildsystems/vcpkg.cmake\"\n</code></pre></p> <p>See Windows Setup Guide for complete instructions.</p>"},{"location":"installation/#macos-quick-start","title":"macOS Quick Start","text":"<pre><code># Install dependencies\nbrew install cmake opencv\n\n# Download ONNX Runtime (Apple Silicon)\nwget https://github.com/microsoft/onnxruntime/releases/download/v1.20.1/onnxruntime-osx-arm64-1.20.1.tgz\ntar -xzf onnxruntime-osx-arm64-1.20.1.tgz\n\n# Build\nmkdir build &amp;&amp; cd build\ncmake .. -DONNXRUNTIME_DIR=../onnxruntime-osx-arm64-1.20.1\nmake -j$(sysctl -n hw.ncpu)\n</code></pre> <p>For Intel Macs, use <code>onnxruntime-osx-x86_64-1.20.1.tgz</code>.</p>"},{"location":"models/","title":"Model Guide","text":"<p>Supported models, ONNX export, and optimization for YOLOs-CPP.</p>"},{"location":"models/#supported-models","title":"Supported Models","text":""},{"location":"models/#detection","title":"Detection","text":"Model Params mAP Speed (GPU) YOLOv5n 1.9M 28.0 6.3ms YOLOv8n 3.2M 37.3 6.2ms YOLOv11n 2.6M 39.5 6.5ms YOLO26n 2.5M 40.2 7.1ms"},{"location":"models/#segmentation","title":"Segmentation","text":"Model Params mAP Speed (GPU) YOLOv8n-seg 3.4M 36.7 8.4ms YOLOv11n-seg 2.9M 38.9 8.1ms YOLO26n-seg 2.8M 39.4 8.8ms"},{"location":"models/#pose-estimation","title":"Pose Estimation","text":"Model Params mAP Speed (GPU) YOLOv8n-pose 3.3M 50.4 5.9ms YOLOv11n-pose 2.9M 52.1 5.7ms YOLO26n-pose 2.8M 53.0 6.2ms"},{"location":"models/#obb-oriented-bounding-boxes","title":"OBB (Oriented Bounding Boxes)","text":"Model Params Dataset YOLOv8n-obb 3.1M DOTA YOLOv11n-obb 2.7M DOTA YOLO26n-obb 2.6M DOTA"},{"location":"models/#classification","title":"Classification","text":"Model Params Top-1 Acc YOLOv8n-cls 2.7M 66.6% YOLOv11n-cls 1.6M 70.0% YOLO26n-cls 1.5M 71.2%"},{"location":"models/#exporting-to-onnx","title":"Exporting to ONNX","text":""},{"location":"models/#using-ultralytics","title":"Using Ultralytics","text":"<pre><code>from ultralytics import YOLO\n\n# Load model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export to ONNX\nmodel.export(\n    format=\"onnx\",\n    imgsz=640,\n    opset=12,        # ONNX opset version\n    simplify=False,\n    half=False,\n    dynamic=False,\n    nms=False        # NMS is done in C++\n)\n</code></pre>"},{"location":"models/#export-options","title":"Export Options","text":"Option Value Notes <code>opset</code> 12-17 Use 12 for max compatibility <code>imgsz</code> 640 Match your inference resolution <code>half</code> False FP32 for accuracy (FP16 optional) <code>dynamic</code> False Static shapes for best performance <code>nms</code> False C++ handles NMS"},{"location":"models/#batch-export-script","title":"Batch Export Script","text":"<pre><code>python models/export_onnx.py\n</code></pre>"},{"location":"models/#label-files","title":"Label Files","text":"File Classes Use Case <code>coco.names</code> 80 General detection <code>Dota.names</code> 15 Aerial/satellite OBB <code>imagenet_classes.txt</code> 1000 Classification"},{"location":"models/#model-paths","title":"Model Paths","text":"<pre><code>// Detection\n\"models/yolo11n.onnx\"\n\n// Segmentation\n\"models/yolo11n-seg.onnx\"\n\n// Pose\n\"models/yolo11n-pose.onnx\"\n\n// OBB\n\"models/yolo11n-obb.onnx\"\n\n// Classification\n\"models/yolo11n-cls.onnx\"\n</code></pre>"},{"location":"models/#quantization","title":"Quantization","text":"<p>Quantized models offer: - 2-4x smaller file size - 1.5-2x faster CPU inference - Slight accuracy loss (~1-2% mAP)</p>"},{"location":"models/#quantize-with-onnx","title":"Quantize with ONNX","text":"<pre><code>from onnxruntime.quantization import quantize_dynamic\n\nquantize_dynamic(\n    \"model.onnx\",\n    \"model_int8.onnx\"\n)\n</code></pre> <p>See <code>quantized_models/yolos_quantization.py</code> for examples.</p>"},{"location":"models/#custom-models","title":"Custom Models","text":"<p>To use custom-trained models:</p> <ol> <li>Train with Ultralytics</li> <li>Export to ONNX with compatible settings</li> <li>Create matching label file</li> <li>Load in YOLOs-CPP</li> </ol> <pre><code>yolos::det::YOLODetector detector(\n    \"custom_model.onnx\",\n    \"custom_labels.txt\",\n    true\n);\n</code></pre>"},{"location":"models/#next-steps","title":"Next Steps","text":"<ul> <li>Usage Guide \u2014 API reference</li> <li>Development \u2014 Extend the library</li> </ul>"},{"location":"quantization/","title":"Model Quantization Guide","text":"<p>Optimize models for faster inference and smaller file size.</p>"},{"location":"quantization/#what-is-quantization","title":"What is Quantization?","text":"<p>Quantization converts model weights from 32-bit floating point (FP32) to lower precision formats:</p> Format Size Speed Accuracy FP32 100% Baseline Best FP16 50% 1.5-2x ~Same INT8 25% 2-4x -1-2% mAP"},{"location":"quantization/#quick-start","title":"Quick Start","text":"<pre><code>from onnxruntime.quantization import quantize_dynamic\n\nquantize_dynamic(\n    \"yolo11n.onnx\",\n    \"yolo11n_int8.onnx\"\n)\n</code></pre>"},{"location":"quantization/#dynamic-quantization","title":"Dynamic Quantization","text":"<p>Best for CPU inference. No calibration data needed.</p> <pre><code>from onnxruntime.quantization import quantize_dynamic, QuantType\n\nquantize_dynamic(\n    model_input=\"yolo11n.onnx\",\n    model_output=\"yolo11n_int8.onnx\",\n    weight_type=QuantType.QUInt8\n)\n</code></pre>"},{"location":"quantization/#static-quantization","title":"Static Quantization","text":"<p>Better accuracy with calibration data.</p> <pre><code>from onnxruntime.quantization import quantize_static, CalibrationDataReader\n\nclass YOLOCalibrationReader(CalibrationDataReader):\n    def __init__(self, images_dir, input_name, input_shape):\n        self.images = [...]  # Load calibration images\n        self.input_name = input_name\n        self.input_shape = input_shape\n        self.index = 0\n\n    def get_next(self):\n        if self.index &gt;= len(self.images):\n            return None\n        # Preprocess image\n        data = self.preprocess(self.images[self.index])\n        self.index += 1\n        return {self.input_name: data}\n\ncalibration_reader = YOLOCalibrationReader(\n    \"calibration_images/\",\n    \"images\",\n    [1, 3, 640, 640]\n)\n\nquantize_static(\n    \"yolo11n.onnx\",\n    \"yolo11n_static_int8.onnx\",\n    calibration_reader\n)\n</code></pre>"},{"location":"quantization/#using-quantized-models","title":"Using Quantized Models","text":"<pre><code>// Same API as FP32 models\nyolos::det::YOLODetector detector(\n    \"yolo11n_int8.onnx\",\n    \"coco.names\",\n    false  // CPU (quantized models are CPU-optimized)\n);\n\nauto detections = detector.detect(frame);\n</code></pre>"},{"location":"quantization/#benchmarks","title":"Benchmarks","text":"<p>Tested on Intel i7-12700H (CPU):</p> Model Size Latency mAP YOLOv11n (FP32) 5.4MB 67ms 39.5 YOLOv11n (INT8) 1.8MB 28ms 38.2 YOLOv8n (FP32) 6.2MB 72ms 37.3 YOLOv8n (INT8) 2.1MB 31ms 36.1"},{"location":"quantization/#tips","title":"Tips","text":"<ol> <li>Calibration data matters \u2014 Use 100-500 representative images</li> <li>Test accuracy \u2014 Validate mAP after quantization</li> <li>CPU only \u2014 INT8 is optimized for CPU, not GPU</li> <li>Per-channel \u2014 Better accuracy than per-tensor</li> </ol>"},{"location":"quantization/#script","title":"Script","text":"<p>See <code>quantized_models/yolos_quantization.py</code> for a complete example.</p>"},{"location":"quantization/#next-steps","title":"Next Steps","text":"<ul> <li>Model Guide \u2014 ONNX export</li> <li>Benchmarks \u2014 Performance testing</li> </ul>"},{"location":"usage/","title":"Usage Guide","text":"<p>Complete API reference and code examples for YOLOs-CPP.</p>"},{"location":"usage/#quick-start","title":"Quick Start","text":"<pre><code>#include \"yolos/yolos.hpp\"\n\n// Initialize any detector\nyolos::det::YOLODetector detector(\"model.onnx\", \"labels.txt\", /*gpu=*/true);\n\n// Run inference\nauto detections = detector.detect(frame, /*conf=*/0.25f, /*iou=*/0.45f);\n\n// Visualize\ndetector.drawDetections(frame, detections);\n</code></pre>"},{"location":"usage/#namespace-structure","title":"Namespace Structure","text":"Namespace Purpose <code>yolos::det::</code> Object detection <code>yolos::seg::</code> Instance segmentation <code>yolos::pose::</code> Pose estimation <code>yolos::obb::</code> Oriented bounding boxes <code>yolos::cls::</code> Image classification"},{"location":"usage/#object-detection","title":"Object Detection","text":"<pre><code>#include \"yolos/yolos.hpp\"\n\nyolos::det::YOLODetector detector(\n    \"models/yolo11n.onnx\",\n    \"models/coco.names\",\n    true  // GPU\n);\n\ncv::Mat image = cv::imread(\"image.jpg\");\nauto detections = detector.detect(image, 0.25f, 0.45f);\n\nfor (const auto&amp; det : detections) {\n    std::cout &lt;&lt; det.className &lt;&lt; \": \" &lt;&lt; det.confidence &lt;&lt; std::endl;\n}\n\ndetector.drawDetections(image, detections);\n</code></pre>"},{"location":"usage/#instance-segmentation","title":"Instance Segmentation","text":"<pre><code>yolos::seg::YOLOSegDetector detector(\n    \"models/yolo11n-seg.onnx\",\n    \"models/coco.names\",\n    true\n);\n\nauto segments = detector.segment(image, 0.25f, 0.45f);\ndetector.drawSegmentations(image, segments, 0.5f);  // 50% opacity\n</code></pre>"},{"location":"usage/#pose-estimation","title":"Pose Estimation","text":"<pre><code>yolos::pose::YOLOPoseDetector detector(\n    \"models/yolo11n-pose.onnx\",\n    \"\",  // No labels needed\n    true\n);\n\nauto poses = detector.detect(image, 0.25f, 0.45f);\ndetector.drawPoses(image, poses);\n</code></pre>"},{"location":"usage/#oriented-bounding-boxes","title":"Oriented Bounding Boxes","text":"<pre><code>yolos::obb::YOLOOBBDetector detector(\n    \"models/yolo11n-obb.onnx\",\n    \"models/Dota.names\",\n    true\n);\n\nauto boxes = detector.detect(image, 0.25f, 0.45f);\ndetector.drawOBBs(image, boxes);\n</code></pre>"},{"location":"usage/#image-classification","title":"Image Classification","text":"<pre><code>yolos::cls::YOLOClassifier classifier(\n    \"models/yolo11n-cls.onnx\",\n    \"models/imagenet_classes.txt\",\n    true\n);\n\nauto result = classifier.classify(image);\nstd::cout &lt;&lt; result.className &lt;&lt; \": \" &lt;&lt; result.confidence * 100 &lt;&lt; \"%\" &lt;&lt; std::endl;\n</code></pre>"},{"location":"usage/#video-processing","title":"Video Processing","text":"<pre><code>cv::VideoCapture cap(\"video.mp4\");\ncv::Mat frame;\n\nwhile (cap.read(frame)) {\n    auto detections = detector.detect(frame);\n    detector.drawDetections(frame, detections);\n    cv::imshow(\"Detection\", frame);\n    if (cv::waitKey(1) == 27) break;\n}\n</code></pre>"},{"location":"usage/#camera-stream","title":"Camera Stream","text":"<pre><code>cv::VideoCapture cap(0);\ncap.set(cv::CAP_PROP_FRAME_WIDTH, 1280);\ncap.set(cv::CAP_PROP_FRAME_HEIGHT, 720);\n\ncv::Mat frame;\nwhile (cap.read(frame)) {\n    auto detections = detector.detect(frame);\n    detector.drawDetections(frame, detections);\n    cv::imshow(\"Live\", frame);\n    if (cv::waitKey(1) == 27) break;\n}\n</code></pre>"},{"location":"usage/#performance-tips","title":"Performance Tips","text":"<ol> <li>Reuse detector instances \u2014 Create once, infer many times</li> <li>Use GPU when available \u2014 5-10x faster than CPU</li> <li>Adjust thresholds \u2014 Higher confidence = fewer detections, faster NMS</li> <li>Match input resolution \u2014 Use model's expected size (640x640)</li> </ol>"},{"location":"usage/#error-handling","title":"Error Handling","text":"<pre><code>try {\n    yolos::det::YOLODetector detector(\"model.onnx\", \"labels.txt\", true);\n} catch (const Ort::Exception&amp; e) {\n    std::cerr &lt;&lt; \"ONNX error: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre>"},{"location":"usage/#next-steps","title":"Next Steps","text":"<ul> <li>Model Guide \u2014 Export and optimize models</li> <li>Development \u2014 Extend the library</li> </ul>"},{"location":"guides/yolos/","title":"Model Guide","text":"<p>Supported models, ONNX export, and optimization for YOLOs-CPP.</p>"},{"location":"guides/yolos/#supported-models","title":"Supported Models","text":""},{"location":"guides/yolos/#detection","title":"Detection","text":"Model Params mAP Speed (GPU) YOLOv5n 1.9M 28.0 6.3ms YOLOv8n 3.2M 37.3 6.2ms YOLOv11n 2.6M 39.5 6.5ms YOLO26n 2.5M 40.2 7.1ms"},{"location":"guides/yolos/#segmentation","title":"Segmentation","text":"Model Params mAP Speed (GPU) YOLOv8n-seg 3.4M 36.7 8.4ms YOLOv11n-seg 2.9M 38.9 8.1ms YOLO26n-seg 2.8M 39.4 8.8ms"},{"location":"guides/yolos/#pose-estimation","title":"Pose Estimation","text":"Model Params mAP Speed (GPU) YOLOv8n-pose 3.3M 50.4 5.9ms YOLOv11n-pose 2.9M 52.1 5.7ms YOLO26n-pose 2.8M 53.0 6.2ms"},{"location":"guides/yolos/#obb-oriented-bounding-boxes","title":"OBB (Oriented Bounding Boxes)","text":"Model Params Dataset YOLOv8n-obb 3.1M DOTA YOLOv11n-obb 2.7M DOTA YOLO26n-obb 2.6M DOTA"},{"location":"guides/yolos/#classification","title":"Classification","text":"Model Params Top-1 Acc YOLOv8n-cls 2.7M 66.6% YOLOv11n-cls 1.6M 70.0% YOLO26n-cls 1.5M 71.2%"},{"location":"guides/yolos/#exporting-to-onnx","title":"Exporting to ONNX","text":""},{"location":"guides/yolos/#using-ultralytics","title":"Using Ultralytics","text":"<pre><code>from ultralytics import YOLO\n\n# Load model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Export to ONNX\nmodel.export(\n    format=\"onnx\",\n    imgsz=640,\n    opset=12,        # ONNX opset version\n    simplify=False,\n    half=False,\n    dynamic=False,\n    nms=False        # NMS is done in C++\n)\n</code></pre>"},{"location":"guides/yolos/#export-options","title":"Export Options","text":"Option Value Notes <code>opset</code> 12-17 Use 12 for max compatibility <code>imgsz</code> 640 Match your inference resolution <code>half</code> False FP32 for accuracy (FP16 optional) <code>dynamic</code> False Static shapes for best performance <code>nms</code> False C++ handles NMS"},{"location":"guides/yolos/#batch-export-script","title":"Batch Export Script","text":"<pre><code>python models/export_onnx.py\n</code></pre>"},{"location":"guides/yolos/#label-files","title":"Label Files","text":"File Classes Use Case <code>coco.names</code> 80 General detection <code>Dota.names</code> 15 Aerial/satellite OBB <code>imagenet_classes.txt</code> 1000 Classification"},{"location":"guides/yolos/#model-paths","title":"Model Paths","text":"<pre><code>// Detection\n\"models/yolo11n.onnx\"\n\n// Segmentation\n\"models/yolo11n-seg.onnx\"\n\n// Pose\n\"models/yolo11n-pose.onnx\"\n\n// OBB\n\"models/yolo11n-obb.onnx\"\n\n// Classification\n\"models/yolo11n-cls.onnx\"\n</code></pre>"},{"location":"guides/yolos/#quantization","title":"Quantization","text":"<p>Quantized models offer: - 2-4x smaller file size - 1.5-2x faster CPU inference - Slight accuracy loss (~1-2% mAP)</p>"},{"location":"guides/yolos/#quantize-with-onnx","title":"Quantize with ONNX","text":"<pre><code>from onnxruntime.quantization import quantize_dynamic\n\nquantize_dynamic(\n    \"model.onnx\",\n    \"model_int8.onnx\"\n)\n</code></pre> <p>See <code>quantized_models/yolos_quantization.py</code> for examples.</p>"},{"location":"guides/yolos/#custom-models","title":"Custom Models","text":"<p>To use custom-trained models:</p> <ol> <li>Train with Ultralytics</li> <li>Export to ONNX with compatible settings</li> <li>Create matching label file</li> <li>Load in YOLOs-CPP</li> </ol> <pre><code>yolos::det::YOLODetector detector(\n    \"custom_model.onnx\",\n    \"custom_labels.txt\",\n    true\n);\n</code></pre>"}]}